<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>

        <!-- Basic Page Needs
================================================== -->
        <meta charset="utf-8">
  <title>ai-doom-posterity</title>
        <meta name="description" content>
        <meta name="author" content>

        <!-- Fonts
================================================== -->
  <link href="http://fonts.googleapis.com/css?family=PT+Sans" rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet" type="text/css">

        <!-- Mobile Specific Metas
================================================== -->
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- CSS
================================================== -->
  <link rel="stylesheet" href="../../css/combined.css">

  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

        <!--[if lt IE 9]>
                <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->

        <!-- Atom
        ================================================== -->
</head>
<body>

        <!-- Primary Page Layout
        ================================================== -->
  <div class="container">
  <div class="ten columns offset-by-three">
  <h1 style="margin-top: 40px"><a class="homelink" href="../../">The H2 Wiki</a></h1>
  <hr />
</div>
<div class="twelve columns offset-by-two">
  <h3>ai-doom-posterity</h3>
  <h1 id="ai-doom-posterity">AI doom posterity</h1>
<p>Will these AI doomers be proved right? Only time will tell (or
won’t)!</p>
<blockquote>
<p>the more I look at the LLM revolution the more worried I am that we
should be counting in months rather than years</p>
</blockquote>
<p>– <a href="https://twitter.com/RokoMijic/status/1643959006040293377">Roko
Mijic</a>,
2023-04-06</p>
<blockquote>
<p>I’ve unfortunately updated towards the singularity happening
sooner. ~2030 say.</p>
</blockquote>
<p>– <a href="https://twitter.com/RokoMijic/status/1642659846716633089">Roko
Mijic</a>,
2023-04-02</p>
<blockquote>
<p>we just had a little baby, and I keep asking myself… how old is he
even gonna get?</p>
</blockquote>
<p>– <a href="https://twitter.com/liron/status/1648185583938969600">Max
Tegmark</a>,
2023-04-18</p>
<blockquote>
<p>As bad as climate change is, AI is likely to get us first I think -
we may only have 2-5 years (vs 20-50 with climate). We need to slam
the brakes on AGI development now.</p>
</blockquote>
<p>– <a href="https://twitter.com/gcolbourn/status/1648267236019167232">Greg
Colbourn</a>,
2023-04-18</p>
<blockquote>
<p>If we go back to things like the bio weapons or cyber [attacks], you
can have really very dangerous threats to humans that could kill
many humans – not all humans – simply from where we would expect
models to be in two years’ time.</p>
</blockquote>
<p>– <a href="https://www.independent.co.uk/news/uk/politics/ai-artificial-intelligence-kill-humans-sunak-b2352099.html">Matt
Clifford</a>,
2023-06-06</p>
<blockquote>
<p>The top scientists at the biggest AI firms believe that they can
make artificial intelligence a billion times more powerful than
today’s most advanced models, creating “something like a god” within
five years</p>
</blockquote>
<p>– <a href="https://twitter.com/DanielColson6/status/1691598172513296710">Daniel
Colson</a>
2023-08-16</p>
<blockquote>
<p>There is a 4%-20% chance of “straight up catastrophe” during 2024.</p>
</blockquote>
<p>– A conclusion from a survey by <a href="https://twitter.com/NickEMoran/status/1767247012825571776">Edouard
Harris</a></p>
<h2 id="non-doom">Non-doom</h2>
<blockquote>
<p>I’m willing to bet we’ll reach 30% unemployment in five years.</p>
</blockquote>
<p>–
<a href="https://www.themotte.org/post/440/culture-war-roundup-for-the-week/87511?context=8#context">TheDag</a>,
2023-04-15</p>
<blockquote>
<p>AGI will absolutely be achieved in AT MOST 3 years. Probably less. I
guarantee. And it will be able to write any imaginable software
faster than the fastest human dev team in the world could, for
pennies.</p>
</blockquote>
<p>– <a href="https://twitter.com/VictorTaelin/status/1747674769342738587">Victor
Taelin</a>,
2024-01-17</p>
<blockquote>
<p>These next three years might be the last few years that I
work. … I stand at the edge of a technological development that
seems likely, should it arrive, to end employment as I know it.</p>
</blockquote>
<p>– <a href="https://www.palladiummag.com/2024/05/17/my-last-five-years-of-work/">Avital
Balwit</a>,
2024-05-17</p>
<blockquote>
<p>Developers, you are f— as a profession. Completely and utterly
f—.</p>
</blockquote>
<p>– <a href="https://x.com/headinthebox/status/1825898022535917697">Erik Meijer</a>,
2024-08-21</p>
<h2 id="ai-notes">AI notes</h2>
<ul>
<li><p>Yudkowsky can’t make predictions about what the trajectory to AI
doom will look like because one can only know the end state not the
trajectory, yet he claims that the trajectory in the last two
decades is evidence that we’re converging on the end state.</p>
<p><a href="https://twitter.com/dwarkesh_sp/status/1643986757174837252" class="uri">https://twitter.com/dwarkesh_sp/status/1643986757174837252</a></p>
<p><a href="https://twitter.com/liron/status/1646301141196742656" class="uri">https://twitter.com/liron/status/1646301141196742656</a></p></li>
</ul>
</div>
<hr>

  </div>

<!-- End Document
================================================== -->
</body>
</html>
